# Dokumentation: Transcibio (Seite 1/3)

## 1. Einleitung und Projektziel

**Transcibio** ist eine Demonstrator-Anwendung, die im Rahmen des Forschungsprojekts **AI Traqc** an der **Hochschule Heilbronn** entwickelt wurde. Das Hauptziel des Projekts ist es, die F√§higkeiten moderner KI-Modelle im Bereich der Audioverarbeitung aufzuzeigen und praktisch erlebbar zu machen.

Die Anwendung konzentriert sich auf eine vollst√§ndig **lokale Verarbeitung** von Audiodaten, um Datenschutz und Unabh√§ngigkeit von Cloud-Diensten zu gew√§hrleisten.

### Kernfunktionen

*   **Automatische Transkription:** Umwandlung von gesprochener Sprache in Text mithilfe von OpenAIs Whisper-Modell.
*   **Sprecher-Diarisierung:** Erkennung und Zuordnung von Sprachsegmenten zu verschiedenen Sprechern ("Wer hat wann gesprochen?"). Dies wird durch `pyannote.audio` realisiert.
*   **KI-gest√ºtzte Zusammenfassung:** Erstellung von Inhaltszusammenfassungen der Transkripte durch ein lokal betriebenes Large Language Model (LLM) √ºber LM Studio.

---

## 2. Bedienungsanleitung

Die Benutzeroberfl√§che ist in einen Hauptbereich f√ºr die Interaktion und eine Seitenleiste f√ºr die Konfiguration unterteilt.

### Schritt 1: Konfiguration in der Seitenleiste

Bevor Sie Audio verarbeiten, nehmen Sie die gew√ºnschten Einstellungen in der linken Seitenleiste vor:

1.  **Transcription & Diarization:**
    *   **Whisper Model:** W√§hlen Sie ein Whisper-Modell. Gr√∂√üere Modelle (`large`) sind genauer, aber langsamer. Kleinere Modelle (`base`, `small`) sind schneller.
    *   **Number of Speakers:** Geben Sie die bekannte Anzahl der Sprecher an. Wenn die Anzahl unbekannt ist, belassen Sie den Wert bei `0` f√ºr eine automatische Erkennung.

2.  **Hugging Face (f√ºr Diarisierung):**
    *   F√ºr die Sprecher-Diarisierung wird ein Hugging Face Token ben√∂tigt.
    *   Wenn das Token nicht automatisch gefunden wird, geben Sie es manuell in das passwortgesch√ºtzte Feld ein. Ohne Token ist die Diarisierung deaktiviert.

3.  **Summarization (LM Studio):**
    *   Stellen Sie sicher, dass **LM Studio** auf Ihrem Computer l√§uft, ein Modell geladen und der Server gestartet ist.
    *   **LM Studio API URL:** Geben Sie die URL Ihres LM-Studio-Servers ein (standardm√§√üig `http://localhost:1234/v1`).
    *   **Local Model Name:** Tragen Sie den Modell-Identifier ein, wie er in LM Studio angezeigt wird.

4.  **Summary Type:**
    *   W√§hlen Sie das gew√ºnschte Format f√ºr die Zusammenfassung (z. B. Standard, Meeting-Protokoll).

### Schritt 2: Audioeingabe im Hauptbereich

W√§hlen Sie eine der beiden Methoden zur Audiobereitstellung:

*   **Upload File:** Laden Sie eine existierende `.wav`-Audiodatei hoch.
*   **Record Audio:** Nehmen Sie live Audio √ºber Ihr Mikrofon auf.
    *   **Einverst√§ndnis:** Bei der ersten Nutzung m√ºssen Sie die **Datenschutzhinweise** lesen und alle Checkboxen aktivieren, um Ihre Zustimmung zu geben.
    *   **Aufnahme:** Klicken Sie auf das Mikrofonsymbol, um die Aufnahme zu starten und erneut, um sie zu beenden.

### Schritt 3: Verarbeitung starten

1.  Klicken Sie auf den Button **"üìä Process Audio"**. Die Verarbeitung kann je nach Audiol√§nge und gew√§hltem Modell einige Zeit in Anspruch nehmen.
2.  Nach Abschluss wird das **sprecher-zugeordnete Transkript** im Hauptbereich angezeigt.

### Schritt 4: Zusammenfassung erstellen

1.  Wenn die Transkription erfolgreich war und LM Studio korrekt konfiguriert ist, erscheint der Button **"Generate Summary"**.
2.  Klicken Sie darauf, um eine Zusammenfassung des Transkripts zu erstellen. Das Ergebnis wird darunter angezeigt. 


## 3. Architektur und Anwendungsstruktur

Die Anwendung ist als monolithischer Service konzipiert, der alle Aufgaben lokal ausf√ºhrt. Die Architektur ist in drei logische Hauptkomponenten gegliedert:

1.  **Benutzeroberfl√§che (Frontend):**
    *   Erstellt mit **Streamlit**, einem Python-Framework zur schnellen Entwicklung von interaktiven Webanwendungen f√ºr Data Science und KI.
    *   Verantwortlich f√ºr die Konfiguration, Dateiuploads, Audioaufnahme und die Darstellung der Ergebnisse.

2.  **Verarbeitungspipeline (Backend-Logik):**
    *   Dies ist das Herzst√ºck der Anwendung und orchestriert die verschiedenen KI-Modelle.
    *   Die Pipeline wird aktiviert, wenn der Benutzer auf "Process Audio" klickt.
    *   Sie folgt einem sequenziellen Ablauf:
        1.  **Audio-Input:** Empf√§ngt die Audiodatei (Upload oder Aufnahme).
        2.  **Diarisierung:** `pyannote.audio` analysiert die Audiodatei und identifiziert die Zeitstempel der verschiedenen Sprecher.
        3.  **Transkription:** Das Whisper-Modell transkribiert den gesamten Audioinhalt zu Text.
        4.  **Alignierung:** Die Ergebnisse aus Diarisierung und Transkription werden zusammengef√ºhrt, um jedem Textsegment einen Sprecher zuzuordnen.

3.  **Zusammenfassung (LLM-Integration):**
    *   Diese Komponente ist von der Haupt-Pipeline entkoppelt.
    *   Sie kommuniziert √ºber eine **HTTP-API** mit dem **LM Studio Server**.
    *   Der transkribierte Text wird in Bl√∂cke (Chunks) aufgeteilt und an das LLM gesendet, um Teil-Zusammenfassungen zu erstellen, die am Ende zu einem Gesamttext kombiniert werden.

### Dateistruktur

Die Codebasis ist wie folgt organisiert:

*   `app.py`: Die Hauptdatei, die die Streamlit-Anwendung und die Benutzeroberfl√§che definiert.
*   `src/`: Ein Verzeichnis, das die Kernlogik enth√§lt, aufgeteilt in Module:
    *   `processing.py`: Funktionen f√ºr Diarisierung, Transkription und Alignierung.
    *   `summarization.py`: Logik f√ºr die Kommunikation mit LM Studio und die Erstellung von Zusammenfassungen.
    *   `utils.py`: Hilfsfunktionen (z. B. Speichern von Dateien).
*   `requirements.txt`: Listet alle Python-Abh√§ngigkeiten des Projekts auf.
*   `consent.md`: Enth√§lt den Text f√ºr die Einverst√§ndniserkl√§rung.

---

## 4. Verwendete Technologien

*   **Sprache:** Python 3.9+
*   **Web-Framework:** Streamlit
*   **Transkription:** [Whisper](https://github.com/openai/whisper) (Modelle von OpenAI, ausgef√ºhrt mit `faster-whisper`)
*   **Sprecher-Diarisierung:** [pyannote.audio](https://github.com/pyannote/pyannote-audio) (erfordert ein Hugging Face Token)
*   **Lokales Large Language Model (LLM):** [LM Studio](https://lmstudio.ai/) als Host f√ºr Modelle wie Llama, Mistral etc.
*   **Audioaufnahme:** `audiorecorder` (Streamlit-Komponente)
*   **Abh√§ngigkeitsmanagement:** `pip` und `requirements.txt`
*   **Containerisierung (optional):** Docker 


## 5. Installationsanleitung

Um die Anwendung lokal auszuf√ºhren, sind mehrere Schritte erforderlich. Diese Anleitung setzt voraus, dass **Python 3.9** oder h√∂her sowie **Git** auf Ihrem System installiert sind.

### Schritt 1: Projekt-Repository klonen

√ñffnen Sie ein Terminal und klonen Sie das Repository von GitHub:

```bash
git clone <URL_des_Repositorys>
cd <Name_des_geklonten_Ordners>
```

### Schritt 2: Python-Umgebung einrichten

Es wird dringend empfohlen, eine virtuelle Umgebung zu verwenden, um Abh√§ngigkeitskonflikte zu vermeiden.

```bash
# Erstellen einer virtuellen Umgebung
python -m venv .venv

# Aktivieren der Umgebung
# Windows:
.venv\Scripts\activate
# macOS / Linux:
source .venv/bin/activate
```

### Schritt 3: Abh√§ngigkeiten installieren

Installieren Sie alle ben√∂tigten Python-Pakete mit `pip`:

```bash
pip install -r requirements.txt
```

**Hinweis:** Die Installation von `torch` und `torchaudio` (Abh√§ngigkeiten von `pyannote.audio`) kann je nach System eine Weile dauern und erfordert m√∂glicherweise eine stabile Internetverbindung.

### Schritt 4: Hugging Face Token konfigurieren

Die Sprecher-Diarisierung mit `pyannote.audio` ben√∂tigt eine Authentifizierung bei Hugging Face.

1.  Erstellen Sie ein Konto auf [huggingface.co](https://huggingface.co/).
2.  Akzeptieren Sie die Nutzungsbedingungen f√ºr die Modelle [pyannote/speaker-diarization](https://huggingface.co/pyannote/speaker-diarization-py3.11) und [pyannote/segmentation](https://huggingface.co/pyannote/segmentation-py3.11).
3.  Erstellen Sie ein **Access Token** in Ihren Hugging Face-Profileinstellungen.
4.  **Optional (empfohlen):** Speichern Sie das Token als Umgebungsvariable `HF_TOKEN`, damit die Anwendung es automatisch laden kann. Alternativ k√∂nnen Sie es bei jeder Nutzung manuell in der Seitenleiste der Anwendung eingeben.

### Schritt 5: LM Studio einrichten (f√ºr Zusammenfassungen)

1.  Laden Sie **LM Studio** von [lmstudio.ai](https://lmstudio.ai/) herunter und installieren Sie es.
2.  Suchen und laden Sie in LM Studio ein passendes Modell herunter (z. B. `Mistral`, `Llama`).
3.  Wechseln Sie zum "Local Server"-Tab (Symbol `<->`).
4.  W√§hlen Sie das geladene Modell aus und starten Sie den Server durch Klick auf **"Start Server"**.
5.  Die angezeigte Server-URL (z.B. `http://localhost:1234/v1`) wird in der Transcibio-Anwendung ben√∂tigt.

### Schritt 6: Anwendung starten

Nachdem alle vorherigen Schritte abgeschlossen sind, starten Sie die Streamlit-Anwendung mit folgendem Befehl im Terminal:

```bash
streamlit run app.py
```

Die Anwendung sollte nun in Ihrem Webbrowser unter einer lokalen Adresse (z. B. `http://localhost:8501`) ge√∂ffnet werden. 